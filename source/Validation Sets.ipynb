{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import ValidationDataset, LibriSpeechLoader, VoxCelebLoader, RandomTripletLossDataset, BSILoader\n",
    "from utils import load_deepfake_dataset\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(labels, name, sete, LOADER):\n",
    "    dataset = ValidationDataset(loader=LOADER(labels, lambda x: x, 0))\n",
    "    df = dataset.data_list\n",
    "\n",
    "    # Create a list to hold the new dataset\n",
    "    new_dataset = []\n",
    "\n",
    "    # Group by speaker to get all utterances for each speaker\n",
    "    speaker_groups = df.groupby('speaker')\n",
    "\n",
    "    # Function to get a random utterance from the same speaker\n",
    "    def get_random_same_speaker(speaker, current_utterance):\n",
    "        speaker_data = speaker_groups.get_group(speaker)\n",
    "        same_speaker_utterance = speaker_data[speaker_data['utterance'] != current_utterance].sample(1)['utterance'].values[0]\n",
    "        return same_speaker_utterance\n",
    "\n",
    "    # Function to get a random utterance from a different speaker\n",
    "    def get_random_different_speaker(speaker):\n",
    "        different_speaker = random.choice([spk for spk in df['speaker'].unique() if spk != speaker])\n",
    "        different_speaker_utterance = df[df['speaker'] == different_speaker].sample(1)['utterance'].values[0]\n",
    "        return different_speaker_utterance\n",
    "\n",
    "    # Loop through each row in the dataset\n",
    "    for index, row in df.iterrows():\n",
    "        utterance = row['utterance']\n",
    "        speaker = row['speaker']\n",
    "        \n",
    "        # Get a random utterance from the same speaker\n",
    "        same_speaker_utterance = get_random_same_speaker(speaker, utterance)\n",
    "        new_dataset.append([utterance, same_speaker_utterance, 1])\n",
    "        \n",
    "        # Get a random utterance from a different speaker\n",
    "        different_speaker_utterance = get_random_different_speaker(speaker)\n",
    "        new_dataset.append([utterance, different_speaker_utterance, 0])\n",
    "\n",
    "    # Create a new DataFrame from the new_dataset list\n",
    "    new_df = pd.DataFrame(new_dataset, columns=['utterance', 'utterance_to_check', 'is_same_speaker'])\n",
    "\n",
    "    # Save the new DataFrame to a CSV file\n",
    "    new_df.to_csv(f'../validation_sets/{sete}/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, valid_labels, test_labels = load_deepfake_dataset(\"LibriSpeech\")\n",
    "create(train_labels, \"train\", \"LibriSpeech\", LibriSpeechLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create(valid_labels, \"valid\", \"LibriSpeech\", LibriSpeechLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create(test_labels, \"test\", \"LibriSpeech\", LibriSpeechLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, valid_labels, test_labels = load_deepfake_dataset(\"VoxCeleb\")\n",
    "create(train_labels, \"train\", \"VoxCeleb\", VoxCelebLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create(valid_labels, \"valid\", \"VoxCeleb\", VoxCelebLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create(test_labels, \"test\", \"VoxCeleb\", VoxCelebLoader)\n",
    "# use https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deepfake(labels, name, sete, LOADER):\n",
    "    dataset = RandomTripletLossDataset(loader=LOADER(labels, lambda x: x, 0))\n",
    "    df = dataset.genuine\n",
    "    all = dataset.data_list\n",
    "    all = all[all[\"is_genuine\"] == 0]\n",
    "\n",
    "    # Create a list to hold the new dataset\n",
    "    new_dataset = []\n",
    "\n",
    "    # Group by speaker to get all utterances for each speaker\n",
    "    speaker_groups = df.groupby('speaker')\n",
    "\n",
    "    # Function to get a random utterance from the same speaker\n",
    "    def get_random_same_speaker(speaker, current_utterance):\n",
    "        speaker_data = speaker_groups.get_group(speaker)\n",
    "        same_speaker_utterance = speaker_data[speaker_data['utterance'] != current_utterance].sample(1)['utterance'].values[0]\n",
    "        same_speaker_method_name = speaker_data[speaker_data['utterance'] != current_utterance].sample(1)['method_name'].values[0]\n",
    "        return same_speaker_utterance, same_speaker_method_name\n",
    "\n",
    "    # Function to get a random utterance from a different speaker\n",
    "    def get_random_different_speaker(speaker):\n",
    "        different_speaker_utterance = all[all['speaker'] == speaker].sample(1)['utterance'].values[0]\n",
    "        different_speaker_method_name = all[all['speaker'] == speaker].sample(1)['method_name'].values[0]\n",
    "        return different_speaker_utterance, different_speaker_method_name\n",
    "\n",
    "    # Loop through each row in the dataset\n",
    "    for index, row in df.iterrows():\n",
    "        utterance = row['utterance']\n",
    "        speaker = row['speaker']\n",
    "        method_name = row['method_name']\n",
    "        \n",
    "        # Get a random utterance from the same speaker\n",
    "        same_speaker_utterance, same_speaker_method_name = get_random_same_speaker(speaker, utterance)\n",
    "        new_dataset.append([utterance, method_name, same_speaker_utterance, same_speaker_method_name, 1])\n",
    "        \n",
    "        # Get a random utterance from a different speaker\n",
    "        different_speaker_utterance, different_speaker_method_name = get_random_different_speaker(speaker)\n",
    "        new_dataset.append([utterance, method_name, different_speaker_utterance, different_speaker_method_name, 0])\n",
    "\n",
    "    # Create a new DataFrame from the new_dataset list\n",
    "    new_df = pd.DataFrame(new_dataset, columns=['utterance', 'method_name', 'utterance_to_check', 'method_name_to_check', 'is_same_speaker'])\n",
    "\n",
    "    # Save the new DataFrame to a CSV file\n",
    "    new_df.to_csv(f'../validation_sets/{sete}/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, valid_labels, test_labels = load_deepfake_dataset(\"BSI\")\n",
    "create_deepfake(train_labels, \"train\", \"BSI\", BSILoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_deepfake(valid_labels, \"valid\", \"BSI\", BSILoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_deepfake(test_labels, \"test\", \"BSI\", BSILoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
